{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf549d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from utils.edit_distance import generate_phonetic_edits, load_words, generate_1_edit_tokens, generate_2_edit_tokens, tokenize_word, edit_distance\n",
    "from utils.file_utils import get_files_recursively\n",
    "from utils.preprocess import get_suffixes, process_word_suffix, split_sentences, remove_punctuation, clean_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86dd2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLINEKHABAR_PATH = '/home/bibek/projects/scrapeet/_scraped/onlinekhabar'\n",
    "CORPUS_PATH = '/home/bibek/projects/scrapeet/_scraped'\n",
    "VOCAB = set(load_words())\n",
    "SUFFIXES = get_suffixes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf53e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phonetic_match(word):   \n",
    "    if word in VOCAB:\n",
    "        return word\n",
    "    phonetics = [x for x in generate_phonetic_edits(word) if x in VOCAB]\n",
    "    #phonetics = [word]\n",
    "    if len(phonetics) == 1:\n",
    "        # print(phonetics[0], 'present after phonetic edits', word)\n",
    "        return phonetics[0]\n",
    "    return word\n",
    "\n",
    "    \n",
    "def get_correct_word(word, pre_existing = {}):\n",
    "    #if word in pre_existing:\n",
    "    #    return pre_existing[word]\n",
    "    \n",
    "    if word in VOCAB:\n",
    "        # pre_existing[word] = word\n",
    "        return word\n",
    "    '''\n",
    "    phonetic_match = get_phonetic_match(word)\n",
    "    if phonetic_match:\n",
    "        pre_existing[word] = phonetic_match\n",
    "        return phonetic_match\n",
    "    '''\n",
    "    \n",
    "    word_suffix = process_word_suffix(SUFFIXES, word)\n",
    "    if not word_suffix:\n",
    "        #pre_existing[word] = word\n",
    "        return word\n",
    "    if word_suffix[0] in VOCAB:\n",
    "        # pre_existing[word] = word\n",
    "        return word\n",
    "    phonetic_match = get_phonetic_match(word_suffix[0])\n",
    "    if phonetic_match:\n",
    "        ret = phonetic_match + ''.join(word_suffix[1:])\n",
    "        #pre_existing[word] = ret\n",
    "        return ret\n",
    "    \n",
    "    # pre_existing[word] = word\n",
    "    return word\n",
    "\n",
    "def get_trigrams(sent):\n",
    "    l = len(sent)\n",
    "    trigrams = []\n",
    "    for x in range(0, l - 3 + 1, 3):\n",
    "        trigrams.append(tuple(sent[x:x+3]))\n",
    "    return trigrams\n",
    "\n",
    "\n",
    "def get_bigrams(sent):\n",
    "    l = len(sent)\n",
    "    bigrams = []\n",
    "    for x in range(1, l-2 + 1, 2):\n",
    "        bigrams.append(tuple(sent[x:x+2]))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a659879c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 29154/29341 [37:20<00:18, 10.03it/s]    "
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "OK_FILES = get_files_recursively(CORPUS_PATH)\n",
    "files_count = len(OK_FILES)\n",
    "WORD_IDS = {}\n",
    "\n",
    "all_sentences = []\n",
    "NEW_VOCAB = Counter()\n",
    "TRIGRAMS = Counter()\n",
    "BIGRAMS = Counter()\n",
    "\n",
    "CHUNK_SIZE = 8\n",
    "\n",
    "async def read_file(fname):\n",
    "    with open(fname) as f:\n",
    "        f.readline()  # the link\n",
    "        return remove_punctuation(f.read())\n",
    "    \n",
    "def process_file(content):\n",
    "    pre_existing_cache = {}\n",
    "    sentences = split_sentences(content)\n",
    "    if sentences[-1].startswith('प्रकाशित'):\n",
    "        sentences = sentences[:-1]\n",
    "    for sentence in sentences:\n",
    "        words = [get_correct_word(clean_word(x), pre_existing_cache) for x in sentence.split() if x]\n",
    "        #NEW_VOCAB.update(words)\n",
    "        gram_words = ['<s>', '<s>', *words, '<e>']\n",
    "        #words.append('<e>')\n",
    "        # For trigrams insert two startings\n",
    "        # words.insert(0, '<s>')\n",
    "        # words.insert(0, '<s>')\n",
    "        return words, get_bigrams(gram_words), get_trigrams(gram_words)\n",
    "        #BIGRAMS.update(get_bigrams(words))\n",
    "        # TRIGRAMS.update(get_trigrams(words))\n",
    "\n",
    "for i in tqdm(range(0, files_count, CHUNK_SIZE)):\n",
    "    processed_contents = await asyncio.gather(*[read_file(OK_FILES[i+x]) for x in range(CHUNK_SIZE) if i+x < files_count])\n",
    "    with Pool(8) as p:\n",
    "        res = p.map(process_file, processed_contents)\n",
    "        # p.join()\n",
    "        for w, bi, tri in res:\n",
    "            NEW_VOCAB.update(w)\n",
    "            BIGRAMS.update(bi)\n",
    "            TRIGRAMS.update(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(NEW_VOCAB))\n",
    "with open('VOCAB_FREQS.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dict(NEW_VOCAB), f, indent=4, ensure_ascii=False)\n",
    "print(\"written new vocab\")\n",
    "\n",
    "print(\"Writing bigrams\")\n",
    "with open('BIGRAMS.txt', 'w', encoding='utf-8') as f:\n",
    "    for k, v in BIGRAMS.items():\n",
    "        f.write(' '.join(k) + \" \" + str(v))\n",
    "        f.write('\\n')\n",
    "print(\"Written bigrams\")\n",
    "\n",
    "print(\"Writing trigrams\")\n",
    "with open('TRIGRAMS.txt', 'w', encoding='utf-8') as f:\n",
    "    for k, v in TRIGRAMS.items():\n",
    "        f.write(' '.join(k) + \" \" + str(v))\n",
    "        f.write('\\n')\n",
    "print(\"Written trigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732ee10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39511\n"
     ]
    }
   ],
   "source": [
    "print(len(FIXED_NEW_VOCAB))\n",
    "with open(\"VOCAB.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(FIXED_NEW_VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae73770f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'त्यहि', 'त्येहि', 'त्येही', 'त्एही', 'त्यही', 'त्एहि'}\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "w = 'त्यहि'\n",
    "print(generate_phonetic_edits(w))\n",
    "from edit_distance import edit_distance\n",
    "\n",
    "print(edit_distance(tokenize_word('त्यहि'), tokenize_word('त्यही')))\n",
    "print(edit_distance(tokenize_word('त्यहि'), tokenize_word('त्यै')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "nn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
