{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "handed-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from preprocess import (\n",
    "    remove_punctuation,\n",
    "    split_sentences,\n",
    "    process_suffixes,\n",
    "    get_suffixes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fantastic-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "CORPUS_PATH = './corpus'\n",
    "SUFFIXES = get_suffixes()\n",
    "WORDS_COUNT_FILE = 'words_count.txt'\n",
    "PAIR_COOCCURENCES_FILE = 'pair_cooccurences.txt'\n",
    "MIN_COUNT_THRESHOLD = 2  # All the tokens with counts less than or equal to this will be ignored from consideration\n",
    "CO_OCCURENCE_WINDOW = 2  # Consider window of two words to consider to have \"co-occured\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "direct-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET CORPUS\n",
    "content_files = os.listdir(CORPUS_PATH)\n",
    "\n",
    "all_content = []\n",
    "\n",
    "for filename in content_files:\n",
    "    path = os.path.join(CORPUS_PATH, filename)\n",
    "    with open(path, 'r') as f:\n",
    "        all_content.append(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "viral-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_extract_sentences(text):\n",
    "    clean = remove_punctuation(text)\n",
    "    sentences = split_sentences(clean)\n",
    "    sentences_word_list = []\n",
    "    for sentence in sentences:\n",
    "        splitted = [x for x in sentence.split() if x]\n",
    "        suffix_processed = process_suffixes(SUFFIXES, splitted)\n",
    "        sentences_word_list.append(\n",
    "            [x for x in suffix_processed if x]\n",
    "        )\n",
    "    return sentences_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "convinced-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43355\n"
     ]
    }
   ],
   "source": [
    "# Get unique words and their corresponding count within corpus\n",
    "words_count = Counter()\n",
    "for content in all_content:\n",
    "    cleaned_sentences = clean_and_extract_sentences(content)\n",
    "    for sentence in cleaned_sentences:\n",
    "        words_count.update(sentence)\n",
    "\n",
    "# Remove low count items\n",
    "words_count = {k: v for k, v in words_count.items() if v > MIN_COUNT_THRESHOLD}\n",
    "print(len(words_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "encouraging-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to sort :  0.01614999771118164\n",
      "WRITTEN\n"
     ]
    }
   ],
   "source": [
    "# Save words list, sorted\n",
    "import time\n",
    "s = time.time()\n",
    "sorted_words_list = sorted(words_count.items(), key=lambda x: x[1], reverse=True)\n",
    "e = time.time()\n",
    "print('Time taken to sort : ', e - s)\n",
    "\n",
    "def write_to_file():\n",
    "    with open(WORDS_COUNT_FILE, 'w') as f:\n",
    "        for k, v in sorted_words_list:\n",
    "            f.write(f'{k} {v}\\n')\n",
    "write_to_file()\n",
    "print('WRITTEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "threaded-watershed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD_IDS LOADED\n"
     ]
    }
   ],
   "source": [
    "# LOAD WORD IDS\n",
    "words_ids = dict()\n",
    "with open(WORDS_COUNT_FILE, 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        word, _ = line.split()\n",
    "        words_ids[word] = i\n",
    "print('WORD_IDS LOADED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "obvious-spain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(3, 4): 2, (3, 5): 2, (1, 3): 2, (1, 2): 1, (2, 3): 1, (4, 5): 1, (2, 4): 1, (1, 5): 1})\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_cooccurences(sentence_word_ids):\n",
    "    occurences = Counter()\n",
    "    for i in range(CO_OCCURENCE_WINDOW):\n",
    "        shifted = sentence_word_ids[i+1:]\n",
    "        pairs = zip(sentence_word_ids, shifted)\n",
    "        sorted_pairs = [tuple(sorted(pair)) for pair in pairs]  # Only work on sorted tuple as (1,2) and (2,1) have same values in the matrix\n",
    "        occurences.update(Counter(sorted_pairs))\n",
    "    return occurences\n",
    "\n",
    "print(get_sentence_cooccurences([1,2,3,4,5,3,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "convertible-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating pair cooccurences\n"
     ]
    }
   ],
   "source": [
    "pair_cooccurences = Counter()\n",
    "# Use the sentences list to create coocrurence matrix\n",
    "for content in all_content:\n",
    "    cleaned_sentences = clean_and_extract_sentences(content)\n",
    "    for sentence in cleaned_sentences:\n",
    "        word_ids = [words_ids[w] for w in sentence if w in words_ids]\n",
    "        occurences = get_sentence_cooccurences(word_ids)\n",
    "        pair_cooccurences.update(occurences)\n",
    "print('DONE creating pair cooccurences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pretty-medicaid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2807077\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b980f4615547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAIR_COOCCURENCES_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw1_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcooccurences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpair_cooccurences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{w1.id} {w2.id} {cooccurences}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Written cooccurences to file {PAIR_COOCCURENCES_FILE}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w1' is not defined"
     ]
    }
   ],
   "source": [
    "# Write pair_cooccurences to a file\n",
    "print(len(pair_cooccurences))\n",
    "with open(PAIR_COOCCURENCES_FILE, 'w') as f:\n",
    "    for (w1_id, w2_id), cooccurences in pair_cooccurences.items():\n",
    "        f.write(f'{w1_id} {w2_id} {cooccurences}\\n')\n",
    "print(f'Written cooccurences to file {PAIR_COOCCURENCES_FILE}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nepali-vectors-embedding",
   "language": "python",
   "name": "nepali-vectors-embedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
