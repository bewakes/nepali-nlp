{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handed-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocess import (\n",
    "    remove_punctuation,\n",
    "    split_sentences,\n",
    "    process_suffixes,\n",
    "    get_suffixes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fantastic-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "CORPUS_PATH = './corpus'\n",
    "SUFFIXES = get_suffixes()\n",
    "WORDS_COUNT_FILE = 'words_count.txt'\n",
    "PAIR_COOCCURENCES_FILE = 'pair_cooccurences.txt'\n",
    "MIN_COUNT_THRESHOLD = 2  # All the tokens with counts less than or equal to this will be ignored from consideration\n",
    "CO_OCCURENCE_WINDOW = 4  # Consider window of two words to consider to have \"co-occured\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "direct-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET CORPUS\n",
    "content_files = os.listdir(CORPUS_PATH)\n",
    "\n",
    "all_content = []\n",
    "\n",
    "for filename in content_files:\n",
    "    path = os.path.join(CORPUS_PATH, filename)\n",
    "    with open(path, 'r') as f:\n",
    "        all_content.append(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "viral-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_extract_sentences(text):\n",
    "    clean = remove_punctuation(text)\n",
    "    sentences = split_sentences(clean)\n",
    "    sentences_word_list = []\n",
    "    for sentence in sentences:\n",
    "        splitted = [x for x in sentence.split() if x]\n",
    "        suffix_processed = process_suffixes(SUFFIXES, splitted)\n",
    "        sentences_word_list.append(\n",
    "            [x for x in suffix_processed if x]\n",
    "        )\n",
    "    return sentences_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "convinced-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab size 147230\n"
     ]
    }
   ],
   "source": [
    "# Get unique words and their corresponding count within corpus\n",
    "words_count = Counter()\n",
    "for content in all_content:\n",
    "    cleaned_sentences = clean_and_extract_sentences(content)\n",
    "    for sentence in cleaned_sentences:\n",
    "        words_count.update(sentence)\n",
    "\n",
    "# Remove low count items\n",
    "words_count = {k: v for k, v in words_count.items() if v > MIN_COUNT_THRESHOLD}\n",
    "print('total vocab size', len(words_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "encouraging-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to sort :  0.04410600662231445\n",
      "WRITTEN\n"
     ]
    }
   ],
   "source": [
    "# Save words list, sorted\n",
    "import time\n",
    "s = time.time()\n",
    "sorted_words_list = sorted(words_count.items(), key=lambda x: x[1], reverse=True)\n",
    "e = time.time()\n",
    "print('Time taken to sort : ', e - s)\n",
    "\n",
    "def write_to_file():\n",
    "    with open(WORDS_COUNT_FILE, 'w') as f:\n",
    "        for k, v in sorted_words_list:\n",
    "            f.write(f'{k} {v}\\n')\n",
    "write_to_file()\n",
    "print('WRITTEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "threaded-watershed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD_IDS LOADED\n"
     ]
    }
   ],
   "source": [
    "# LOAD WORD IDS\n",
    "words_ids = dict()\n",
    "with open(WORDS_COUNT_FILE, 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        word, _ = line.split()\n",
    "        words_ids[word] = i\n",
    "print('WORD_IDS LOADED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "obvious-spain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(1, 3): 3, (2, 3): 2, (3, 4): 2, (3, 5): 2, (1, 5): 2, (1, 4): 2, (1, 2): 1, (4, 5): 1, (2, 4): 1, (2, 5): 1, (3, 3): 1})\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_cooccurences(sentence_word_ids):\n",
    "    occurences = Counter()\n",
    "    for i in range(CO_OCCURENCE_WINDOW):\n",
    "        shifted = sentence_word_ids[i+1:]\n",
    "        pairs = zip(sentence_word_ids, shifted)\n",
    "        sorted_pairs = [tuple(sorted(pair)) for pair in pairs]  # Only work on sorted tuple as (1,2) and (2,1) have same values in the matrix\n",
    "        occurences.update(Counter(sorted_pairs))\n",
    "    return occurences\n",
    "\n",
    "print(get_sentence_cooccurences([1,2,3,4,5,3,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "convertible-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating pair cooccurences\n"
     ]
    }
   ],
   "source": [
    "pair_cooccurences = Counter()\n",
    "# Use the sentences list to create coocrurence matrix\n",
    "for content in all_content:\n",
    "    cleaned_sentences = clean_and_extract_sentences(content)\n",
    "    for sentence in cleaned_sentences:\n",
    "        word_ids = [words_ids[w] for w in sentence if w in words_ids]\n",
    "        occurences = get_sentence_cooccurences(word_ids)\n",
    "        pair_cooccurences.update(occurences)\n",
    "print('DONE creating pair cooccurences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "round-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18509970\n",
      "Written cooccurences to file pair_cooccurences.txt\n"
     ]
    }
   ],
   "source": [
    "# Write pair_cooccurences to a file\n",
    "print(len(pair_cooccurences))\n",
    "with open(PAIR_COOCCURENCES_FILE, 'w') as f:\n",
    "    for (w1_id, w2_id), cooccurences in pair_cooccurences.items():\n",
    "        f.write(f'{w1_id} {w2_id} {cooccurences}\\n')\n",
    "print(f'Written cooccurences to file {PAIR_COOCCURENCES_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frank-nicholas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WORD_IDS\n",
      "LOADED WORD_IDS.\n",
      "LOADING COOCCURRENCES...\n",
      "LOADED COOCCURRENCES.\n"
     ]
    }
   ],
   "source": [
    "## NOW all the data is processed, we just need to load cooccurences and words_ids files to work with creating vector embeddings\n",
    "\n",
    "print('LOADING WORD_IDS')\n",
    "word_ids = {}\n",
    "word_ids_reverse = {}\n",
    "with open(WORDS_COUNT_FILE, 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        word, _ = line.split()\n",
    "        word_ids[word] = i\n",
    "        word_ids_reverse[i] = word\n",
    "print('LOADED WORD_IDS.')\n",
    "\n",
    "\n",
    "cooccurrences_symmetric = dict()\n",
    "print('LOADING COOCCURRENCES...')\n",
    "with open(PAIR_COOCCURENCES_FILE, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        w1, w2, occ = line.split()\n",
    "        pair = tuple(sorted([int(w1), int(w2)]))\n",
    "        cooccurrences_symmetric[pair] = int(occ)\n",
    "print('LOADED COOCCURRENCES.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "economic-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18509970/18509970 [00:09<00:00, 2034137.48it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_standard_deviations(cooccurrences_symmetric, word_ids):\n",
    "    \"\"\"\n",
    "    Get variance of given dimension(here each word contributes as a dimension)\n",
    "    This will later be used to filter out non varying dimensions\n",
    "    @parameters:\n",
    "        - cooccurrences_symmetric: cooccurence matrix\n",
    "        - word_ids: words(dimensions)\n",
    "    @returns: a float denoting the variance\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_ids)\n",
    "    word_cols = {v: [] for v in word_ids.values()}\n",
    "    word_mean = dict()\n",
    "    for (w1, w2), occ in tqdm(cooccurrences_symmetric.items()):\n",
    "        if w1 == w2:\n",
    "            word_cols[w1].append(occ)\n",
    "        else:\n",
    "            word_cols[w1].append(occ)\n",
    "            word_cols[w2].append(occ)\n",
    "            \n",
    "    for k,v in word_cols.items():\n",
    "        word_mean[k] = sum(v) / vocab_size\n",
    "    # Now get stds\n",
    "    stds = dict()\n",
    "    for k, cols in word_cols.items():\n",
    "        count = len(cols)\n",
    "        diff = vocab_size - count  # to account for all the zero values not included in the cols list\n",
    "        sq_sum = sum([(v - word_mean[k])**2 for v in cols]) + diff * (word_mean[k]**2)\n",
    "        var = (sq_sum / vocab_size)**0.5\n",
    "        stds[k] = var\n",
    "    \n",
    "    return stds\n",
    "\n",
    "stds = get_standard_deviations(cooccurrences_symmetric, word_ids)\n",
    "\n",
    "# sort by min\n",
    "sorted_stds = sorted(stds.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "driving-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147230 73615000 147230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('नवनिर्मित', 0.3874627534666715),\n",
       " ('बर्साइन', 0.38718611837237554),\n",
       " ('भयानक', 0.38714738253642256),\n",
       " ('स्त्रिन', 0.3870930393098868),\n",
       " ('गर्नू', 0.3870812525369398),\n",
       " ('रेवती', 0.38704020214950824),\n",
       " ('मिश्रित', 0.3870341628238483),\n",
       " ('बीस', 0.3870311741166209),\n",
       " ('मेघी', 0.3867622941925958),\n",
       " ('चारबुँदे', 0.38675423133903564),\n",
       " ('न्यारोबडी', 0.3867246360432328),\n",
       " ('मम', 0.3866943777413174),\n",
       " ('लाग्ला', 0.3866382618096648),\n",
       " ('बिलिङ', 0.38657419940827453),\n",
       " ('प्रत्यायोजित', 0.3865710128630104),\n",
       " ('मिलाउनुपर्ने', 0.3865617894314184),\n",
       " ('गल्फ', 0.3865302752298907),\n",
       " ('मेघावाट', 0.3864434020711244),\n",
       " ('टंक', 0.38631408224465547),\n",
       " ('बगेर', 0.38627667126786824),\n",
       " ('अधिराज्य', 0.3861821416368391),\n",
       " ('गरेवापत', 0.3861721097235041),\n",
       " ('ब्याण्ड', 0.38599366213708264),\n",
       " ('नकआउट', 0.3859838908373834),\n",
       " ('अम्बि', 0.38597430741847133),\n",
       " ('दिन्न', 0.3857032649748895),\n",
       " ('भजनी', 0.38565279379383405),\n",
       " ('बुद्धिजीवी', 0.3856401423684163),\n",
       " ('काग्रेस', 0.385571460231522),\n",
       " ('थममाया', 0.38556475783692484),\n",
       " ('कुलबहादुर', 0.385501695840609),\n",
       " ('पीआर', 0.38549197549682473),\n",
       " ('आउनुस्', 0.3854872948028581),\n",
       " ('गर्दिनँ', 0.3854192827686952),\n",
       " ('तोडेर', 0.38535861643865754),\n",
       " ('सावित्रा', 0.3852983353131253),\n",
       " ('भीमदत्तनगर', 0.38522522678174914),\n",
       " ('सुनौलो', 0.38522025764715667),\n",
       " ('अवलम्वन', 0.38520445428568867),\n",
       " ('एफआई', 0.385167479120273),\n",
       " ('बुझेन', 0.385167323176211),\n",
       " ('दवाडी', 0.3851344420030246),\n",
       " ('सच्याएर', 0.3851272990396985),\n",
       " ('पान', 0.385033047718111),\n",
       " ('रुपैयां', 0.38503173041537353),\n",
       " ('बर्', 0.3848768057103833),\n",
       " ('उभिन', 0.38483866816547596),\n",
       " ('टाँगेर', 0.38482510017246147),\n",
       " ('बचाऊ', 0.3848234081931397),\n",
       " ('जीटूजी', 0.3848167421609535),\n",
       " ('दिलेन्द्रप्रसाद', 0.3847861902588326),\n",
       " ('हतारो', 0.38476639472643054),\n",
       " ('कहिँ', 0.38468219346659227),\n",
       " ('साँठगाँठ', 0.3845780898604913),\n",
       " ('लो', 0.3845696873140367),\n",
       " ('किस्ताबापत', 0.3845455260610938),\n",
       " ('देवराज', 0.3845266145732388),\n",
       " ('जवाफी', 0.384382005888176),\n",
       " ('भिड्दै', 0.3843674086709684),\n",
       " ('प्रतिप्रश्न', 0.38433803978851205),\n",
       " ('एकमत', 0.38430743980682297),\n",
       " ('मान्दैनन्', 0.38429953576452747),\n",
       " ('तेक्वान्दो', 0.38425466910516004),\n",
       " ('प्यारो', 0.3842140017174971),\n",
       " ('समययता', 0.3841997875785202),\n",
       " ('बाल्न', 0.38417634949058754),\n",
       " ('पार्टस', 0.38417308061322125),\n",
       " ('मानेन', 0.3841436145508433),\n",
       " ('काफी', 0.38412934496488993),\n",
       " ('अड्किए', 0.3841211070460151),\n",
       " ('पहेंलो', 0.38408579344962696),\n",
       " ('टुटे', 0.3840377536557841),\n",
       " ('लुटेरा', 0.3840201830483745),\n",
       " ('अनुपस्थिति', 0.38385530306333543),\n",
       " ('फैलाए', 0.38375541224129617),\n",
       " ('थाई', 0.38370353088410397),\n",
       " ('मिश्रति', 0.38369759053161573),\n",
       " ('झपट', 0.38360947307913523),\n",
       " ('सुस्ताए', 0.3834020063588566),\n",
       " ('मोनो', 0.3833281752376147),\n",
       " ('ब्यापारिक', 0.3832760124126216),\n",
       " ('भत्किएर', 0.38324464078282977),\n",
       " ('उर्दी', 0.38316068929164676),\n",
       " ('दृष्टान्त', 0.3831209874469517),\n",
       " ('क्लोज', 0.38305833292448316),\n",
       " ('आइओसी', 0.38297232658056246),\n",
       " ('दरवन्दी', 0.3829286571481478),\n",
       " ('कटौति', 0.3829140159726273),\n",
       " ('खेपिरहे', 0.3828928068192644),\n",
       " ('चेन्नाई', 0.3827594027292953),\n",
       " ('रामदेव', 0.38264438174610876),\n",
       " ('छड', 0.38244142175238244),\n",
       " ('परकम्प', 0.38236678609791713),\n",
       " ('भौचर', 0.38236598709265857),\n",
       " ('विकराल', 0.38213512202936767),\n",
       " ('डाइट', 0.3820543243137328),\n",
       " ('हेलम्बु', 0.38204826153667076),\n",
       " ('ककनी', 0.3819954330770042),\n",
       " ('डिला', 0.3819923943795623),\n",
       " ('फालिए', 0.3819252188806008)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_ids), len(word_ids) * 500, len(sorted_stds))\n",
    "[(word_ids_reverse[w], std) for w, std in sorted_stds[9900:10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "million-welsh",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff7d63a9048>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot standard deviations\n",
    "choosen = sorted_stds[:]\n",
    "std_vals = np.array([v for _, v in choosen])\n",
    "plt.plot(np.arange(len(choosen)), std_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crazy-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147230/147230 [09:18<00:00, 263.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Consider only certain number of dimensions with highest standard deviations\n",
    "N_DIMS_SELECTED = 5000\n",
    "choosen_dimensions = {k for k, _ in sorted_stds[:N_DIMS_SELECTED]}\n",
    "\n",
    "# NOW create sparse matrix of all words and their vector representations\n",
    "all_words_embeds = dict()\n",
    "for v in tqdm(word_ids.values()):\n",
    "    vector = []\n",
    "    for dim in choosen_dimensions:\n",
    "        pair = tuple(sorted([v, dim]))\n",
    "        vector.append(cooccurrences_symmetric.get(pair, 0))\n",
    "    all_words_embeds[v] = vector\n",
    "print('DONE!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# write dimensions and word vectors to pkl file\n",
    "RAW_VECTORS_PATH = 'raw_vectors.pkl'\n",
    "with open(RAW_VECTORS_PATH, 'wb') as f:\n",
    "    d = {'dimensions': list(choosen_dimensions)}\n",
    "    d['word_embeddings'] = sparse.csr_matrix([v for k, v in all_words_embeds.items()])\n",
    "    pickle.dump(d, f)\n",
    "print('done writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ RAW vectors\n",
    "with open(RAW_VECTORS_PATH, 'rb') as f:\n",
    "    vectors_data = pickle.load(f)\n",
    "print('LOADED VECTORS')\n",
    "sparse_matrix = vectors_data['word_embeddings']\n",
    "print('CREATED SPARSE MATRIX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform dimension reduction\n",
    "print(sparse_matrix.shape)\n",
    "components = 30\n",
    "tsvd = TruncatedSVD(n_components=components)\n",
    "tsvd.fit(sparse_matrix)\n",
    "plt.plot(np.arange(1, components+1), tsvd.explained_variance_ratio_)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dimension reduced data to file\n",
    "transformed = tsvd.transform(sparse_matrix)\n",
    "print(transformed.shape)\n",
    "with open('reduced_data_ndarray.pkl', 'wb') as f:\n",
    "    pickle.dump(transformed, f)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD dim reduced data\n",
    "with open('reduced_data_ndarray.pkl', 'rb') as f:\n",
    "    data_array = pickle.load(f)\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for fonts\n",
    "# Source: https://jdhao.github.io/2018/04/08/matplotlib-unicode-character/\n",
    "\n",
    "from fontTools.ttLib import TTFont\n",
    "import matplotlib.font_manager as mfm\n",
    "\n",
    "def char_in_font(Unicode_char, font):\n",
    "    for cmap in font['cmap'].tables:\n",
    "        if cmap.isUnicode():\n",
    "            if ord(Unicode_char) in cmap.cmap:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "uni_char =  u\"ल\"\n",
    "# or uni_char = u\"\\u2739\"\n",
    "\n",
    "font_info = [(f.fname, f.name) for f in mfm.fontManager.ttflist]\n",
    "\n",
    "for i, font in enumerate(font_info):\n",
    "    try:\n",
    "        if char_in_font(uni_char, TTFont(font[0], fontNumber=0)):\n",
    "            print(font[0], font[1])\n",
    "    except:\n",
    "        print('no font')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add font for plotting\n",
    "plt.figure(figsize=(15, 12))\n",
    "prop = fm.FontProperties(fname='/usr/share/fonts/noto/NotoSansDevanagari-Regular.ttf')\n",
    "# Now reduce to two dimensions and view\n",
    "svd2 = TruncatedSVD(n_components=2)\n",
    "data_2d = svd2.fit_transform(data_array)\n",
    "choosen_indices = [16, 25, 39, 44, 52, 74, 101, 121, 204, 379, 374, 405]\n",
    "annotations = [word_ids_reverse[x] for x in choosen_indices]\n",
    "x = data_2d[choosen_indices, 0]\n",
    "y = data_2d[choosen_indices, 1]\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i], y[i]), fontproperties=prop, fontsize=20)\n",
    "plt.scatter(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nepali-vectors-embedding",
   "language": "python",
   "name": "nepali-vectors-embedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
